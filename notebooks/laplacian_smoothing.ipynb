{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacian smoothing\n",
    "\n",
    "By `@jhmarcus`\n",
    "\n",
    "Here I explore the possible relationship beteween laplacian smoothing and the autoregressive models I've been looking into"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplacian smoothing is a way to smooth out observed values on the nodes of the graph see\n",
    "\n",
    "* http://www.stat.cmu.edu/~cshalizi/networks/16-1/lectures/13/lecture-13.pdf\n",
    "* http://www.stat.cmu.edu/~ryantibs/papers/sparsify.pdf\n",
    "\n",
    "It can be conceived as the following optimization problem ...\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{\\tilde{\\mathbf{y}}} \\in \\mathbb{R}^n}{min} \\ ||\\mathbf{y}-\\tilde{\\mathbf{y}}||^2_2 + \\lambda \\sum_{i,j} w_{ij}(\\tilde{y}_i - \\tilde{y}_j)^2 \n",
    "$$\n",
    " \n",
    "Where $w_{ij}$ is the edge weight. We can represent this in matrix form as ...\n",
    " \n",
    "$$\n",
    "\\underset{\\mathbf{\\tilde{\\mathbf{y}}} \\in \\mathbb{R}^n}{min} \\ ||\\mathbf{y}-\\tilde{\\mathbf{y}}||^2_2 + \\lambda \\tilde{\\mathbf{y}}^T\\mathbf{L} \\tilde{\\mathbf{y}}\n",
    "$$ \n",
    " \n",
    "where $\\mathbf{L}$ is the graph laplacian. It can be shown the solution to this problem is analytical ...\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{y}}^* = (\\mathbf{I} + \\lambda \\mathbf{L})^{-1} \\mathbf{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = (\\mathbf{I} + \\lambda \\mathbf{L})\\tilde{\\mathbf{y}}^*\n",
    "$$\n",
    "\n",
    "Lets now use this to motivate the setup an autoregressive process ...\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = (\\mathbf{I} + \\lambda\\mathbf{L})\\mathbf{y} + \\nu\n",
    "$$\n",
    "\n",
    "where $\\nu \\sim \\mathcal{N}(\\nu|\\mathbf{0}, \\sigma^2\\mathbf{I})$ ...\n",
    "\n",
    "$$\n",
    "\\mathbf{y} - (\\mathbf{I} + \\lambda\\mathbf{L})\\mathbf{y}  = \\nu\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{y}\\big(\\mathbf{I} - (\\mathbf{I} + \\lambda\\mathbf{L})\\big) = \\nu\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\frac{1}{\\lambda}\\mathbf{L}^{-1}\\nu\n",
    "$$\n",
    "\n",
    "This implies that ...\n",
    "\n",
    "$$\n",
    "\\mathbf{y} | \\lambda, \\sigma^2, \\mathbf{L} \\sim \\mathcal{N}\\Big(\\mathbf{y}|\\mathbf{0}, \\frac{\\sigma^2}{\\lambda}(\\mathbf{L}\\mathbf{L}^T)^{-1}\\Big)\n",
    "$$\n",
    "\n",
    "Because $\\lambda$ and $\\sigma^2$ are unidentifiable let $\\tau = \\frac{\\sigma^2}{\\lambda}$ ...\n",
    "\n",
    "$$\n",
    "\\mathbf{y} | \\tau, \\mathbf{L} \\sim \\mathcal{N}\\big(\\mathbf{y}|\\mathbf{0}, \\tau(\\mathbf{L}\\mathbf{L}^T)^{-1}\\big)\n",
    "$$\n",
    "\n",
    "This is exactly the covariance derived in Hanks 2016 from the perspective of a spatio-temporal random-walk. Perhaps this explains why it performs the best in the [isolation_by_srw](https://github.com/jhmarcus/spatial-random-walk/blob/master/notebooks/isolation_by_srw.ipynb) notebook? For more intution lets consider a two deme example (assuming $\\lambda = 1$) ...\n",
    "\n",
    "`todo`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
